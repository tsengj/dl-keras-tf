# See http://text2vec.org/glove.html for more details about text2vec
get_embeddings <- function(text) {
  
  # Create iterator over tokens
  tokens <- text2vec::space_tokenizer(text)
  
  # Create vocabulary. Terms will be unigrams (simple words).
  message("Creating vocabulary...")
  it <- text2vec::itoken(tokens, progressbar = FALSE)
  vocab <- text2vec::create_vocabulary(it)
  vocab <- text2vec::prune_vocabulary(vocab, term_count_min = 5L)
  
  # Use our filtered vocabulary
  vectorizer <- text2vec::vocab_vectorizer(vocab)
  
  # Use window of 5 for context words
  message("Creating term-co-occurence matrix...")
  tcm <- text2vec::create_tcm(it, vectorizer, skip_grams_window = 5L)
  
  # Fit the model
  message("Computing embeddings based on GloVe algorithm...")
  glove <- text2vec::GlobalVectors$new(
    rank = 50, x_max = 20
    )
  wv_main <- glove$fit_transform(tcm, n_iter = 20, convergence_tol = 0.01)
  wv_context = glove$components
  wv_main + t(wv_context)
}

get_similar_words <- function(reference_word, word_embeddings, n_words = 5) {
  
  # Find closest aligned word embeddings based on cosine similarity
  tryCatch({
    word <- word_embeddings[reference_word, , drop = FALSE]
  },
    error = function(e) {
      stop("The supplied word ('", reference_word, "') is not part of the created vocabulary.")
    }
  )
  
  cos_sim <- text2vec::sim2(
    x = word_embeddings, 
    y = word, 
    method = "cosine", 
    norm = "l2"
    )
  
  head(sort(cos_sim[,1], decreasing = TRUE), n_words)
  
}

similar_classification_words <- function(word, word_embeddings, n = 6) {
  similarities <- word_embeddings[word, , drop = FALSE] %>%
    text2vec::sim2(embedding_wts, y = ., method = "cosine")
  
  similarities[,1] %>% sort(decreasing = TRUE) %>% head(n)
}
